import{_ as i}from"./_plugin-vue_export-helper.cdc0426e.js";import{o,c as s,a as r,b as e,d as a,e as t,f as c,r as l}from"./app.3a641fb2.js";const d="/blog/assets/image2.4ef7f6f8.png",h="/blog/assets/image3.c07a8862.jpeg",p="/blog/assets/image1.22b1da06.jpeg",g="/blog/assets/image5.05cf9063.png",m="/blog/assets/image6.3d54ae7b.png",u="/blog/assets/image4.f7b30f44.png",f={},b=e("p",null,"This blog describes a recent work on the challenge of generating long coherent sequences with language models by leveraging goal-conditioned latent paths.",-1),_=e("p",null,"It is based on the paper: Language modeling via stochastic processes by Wang, R. E., Durmus, E., Goodman, N., & Hashimoto, T. (2022).",-1),y=t("Paper: "),x={href:"https://arxiv.org/abs/2203.11370",target:"_blank",rel:"noopener noreferrer"},v=t("https://arxiv.org/abs/2203.11370"),w=t("Code: "),T={href:"https://github.com/rosewang2008/language_modeling_via_stochastic_processes",target:"_blank",rel:"noopener noreferrer"},k=t("https://github.com/rosewang2008/language_modeling_via_stochastic_processes"),C=c('<h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h2><p>Writing a few lines is an easy chore for most individuals, but even seasoned authors frequently run into difficulties when trying to construct their second chapter. A similar problem plagues today\u2019s large-scaled pretrained language models, such as GPT-2, which excel at short text production but degrade into incoherence when used for lengthier texts. The incapacity of such models to plan or reflect long-range dynamics might be blamed for the failure to evolve texts from beginning to conclusion correctly.</p><p><img src="'+d+'" alt="image2"></p><p>Prior work has explored remedies for this failure mode by using planning-based methods or implicitly learning text dynamics. However, these methods manually specify the text dynamics or sacrifice quality in long-horizon generation.</p><p>To address these challenges, a Stanford University research team introduced Time Control (TC), a language model that implicitly plans using a latent stochastic process and seeks to generate sentences that follow this secret plan. Human assessors scored the outputs 28.6 percent higher than baseline approaches, indicating that the unique strategy enhances performance on long text production.</p><h2 id="significant-contributions" tabindex="-1"><a class="header-anchor" href="#significant-contributions" aria-hidden="true">#</a> Significant Contributions</h2><p>The team\u2019s significant contributions are summarised as follows:</p><ol><li>Time Control is a language model derived by the team that explicitly represents latent structure using Brownian bridge dynamics acquired with a new contrastive aim.</li><li>Compared to task-specific approaches, the team showed that Time Control creates more or equally coherent text on tasks such as text infilling and forced lengthy text production across various text domains.</li><li>By evaluating discourse coherence with human studies, The team demonstrates that their latent representations capture text dynamics competitively.</li><li>The relevance of the contrastive aim, enforcing Brownian bridge dynamics, and explicitly modeling latent dynamics are all emphasized in their technique.</li></ol><h2 id="method" tabindex="-1"><a class="header-anchor" href="#method" aria-hidden="true">#</a> Method</h2><p>The proposed TC approach learns a latent space with smooth temporal dynamics for modeling and creating coherent text. The researchers devised a unique contrastive goal for learning a latent space with Brownian bridge dynamics and then utilized this latent space to create text that keeps local coherence while displaying better global coherence.</p><p><img src="'+h+'" alt="image3"></p><p>The TC text generation pipeline uses the Brownian bridge process to plan a latent trajectory with a start and finish, then conditionally creates sentences that follow this latent plan.</p><p>The intuition is simple: The bridge imposes that a positive triplet (eg. three in-order sentences on Boston) makes up a smooth trajectory. A negative triplet should not construct a smooth trajectory (switching middle sentences with one on New York).</p><p>After training the encoder, GPT2 is finetuned to decode from past context and the encoded latent plan. At inference, a latent plan is generated by sampling from the bridge and conditionally generate each sentence using the latent plan.</p><p><img src="'+p+'" alt="image1"></p><h2 id="discussion-and-conclusion" tabindex="-1"><a class="header-anchor" href="#discussion-and-conclusion" aria-hidden="true">#</a> Discussion and Conclusion</h2><p>Four questions were addressed in the team\u2019s empirical study:</p><ol><li>Is it possible to represent local text dynamics using Time Control?</li><li>Is it possible for Time Control to create locally coherent language?</li><li>Is it possible to represent global text dynamics using Time Control?</li><li>Is Time Control capable of producing long, cohesive documents?</li></ol><p>For three tasks: discourse coherence, text-infilling, document structure imitating, and extended text production, they compared TC to domain-specific approaches and fine-tuning on GPT-2 across diverse text domains. Wiki section, TM-2, TicketTalk, and Recipe NLG were among the datasets used in the tests.</p><p><img src="'+g+'" alt="image5"></p><p><img src="'+m+'" alt="image6"></p><p>TC didn\u2019t sacrifice short/mid-range language modeling performance as it improved performance on text infilling and discourse coherence tasks in the tests while preserving text structure for long text generation in terms of ordering (up to +40%) and text length consistency (up to +17%); this demonstrates the proposed method\u2019s ability to generate more locally and globally coherent texts.</p><p><img src="'+u+'" alt="image4"></p><p>According to the team, TC may expand to other domains containing sequential data, such as movies or music, and support arbitrary bridge operations with unknown fixed start and endpoints.</p><h2 id="reference" tabindex="-1"><a class="header-anchor" href="#reference" aria-hidden="true">#</a> Reference</h2><ul><li>Wang, R. E., Durmus, E., Goodman, N., &amp; Hashimoto, T. (2022). Language modeling via stochastic processes. arXiv preprint arXiv:2203.11370.</li></ul>',26);function q(B,N){const n=l("ExternalLinkIcon");return o(),s("div",null,[b,_,r(" more "),e("p",null,[y,e("a",x,[v,a(n)])]),e("p",null,[w,e("a",T,[k,a(n)])]),C])}const G=i(f,[["render",q],["__file","index.html.vue"]]);export{G as default};
